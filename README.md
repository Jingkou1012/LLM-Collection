# LLM-Study


> The limits of my language mean the limites of my world. - Ludwig Wittgenstein

---
## Overview

---
## Transformer
- 170612 - Google - Transformer - Attention is All You Need - [arXiv](https://arxiv.org/abs/1706.03762)
- 181011 - Google - BERT - Bidirectional Encoder Representations from Transformers - [arXiv](https://arxiv.org/abs/1810.04805)
- 201022 - Google - ViT - An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale - [arXiv](https://arxiv.org/abs/2010.11929)
---
## Large Language Model
- 180611 - OpenAI - GPT -  Improving Language Understanding by Generative Pre-Training - [Report](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- 190214 - OpenAI - GPT-2 - Language Models are Unsupervised Multitask Learners - [Report](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- 200528 - OpenAI - GPT-3 - Language Models are Few-Shot Learners - [arXiv](https://arxiv.org/abs/2005.14165)
- 210226 - OpenAI - CLIP - Learning Transferable Visual Models From Natural Language Supervision - [arXiv](https://arxiv.org/abs/2103.00020)
- 230227 - Meta - Llama - Open and Efficient Foundation Language Models - [arXiv](https://arxiv.org/abs/2302.13971)
- 230315 - OpenAI - GPT-4 - GPT-4 Technical Report - [arXiv](https://arxiv.org/abs/2303.08774)
- 230718 - Meta - Llama 2 - Open Foundation and Fine-Tuned Chat Models - [arXiv](https://arxiv.org/abs/2307.09288)
- 231219 - Google - Gemini - A Family of Highly Capable Multimodal Models - [arXiv](https://arxiv.org/abs/2312.11805)
- 240221 - Google - Gemma - Open Models Based on Gemini Research and Technology - [Report](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf)
- 240308 - Google - Gemini 1.5 - Unlocking Multimodal Understanding across Millions of Tokens of Context - [arXiv](https://arxiv.org/abs/2403.05530)
- 240418 - Meta - Llama 3 - Introducing Meta Llama 3: The most capable openly available LLM to date - [blog](https://ai.meta.com/blog/meta-llama-3/)
- 240627 - Google - Gemma 2 - Improving Open Language Models at a Practical Size - [Report](https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf)
---
## Fine-tuning

---
## Retrieval Augmented Generation

---
## Vision Large Model
- 230417 - Haotian Liu - LLaVA - Visual Instruction Tuning - [arXiv](https://arxiv.org/abs/2304.08485)
- 230925 - OpenAI - GPT-4V - GPT-4V(ision) System Card - [Report](https://cdn.openai.com/papers/GPTV_System_Card.pdf)
- 231005 - Haotian Liu - LLaVA 1.5 - Improved Baselines with Visual Instruction Tuning - [arXiv](https://arxiv.org/abs/2310.03744)
- 240130 - Haotian Liu - LLaVA-NeXT - Improved Reasoning, OCR, and World Knowledge - [blog](https://llava-vl.github.io/blog/2024-01-30-llava-next/)
- 240514 - Google - PaliGemma - Google's Cutting-Edge Open Vision Language Model - [blog](https://huggingface.co/blog/paligemma)
---
## Autonomous Driving
- 221220 - OpenDriveLab - UniAD - Planning-Oriented Autonomous Driving - [arXiv](https://arxiv.org/abs/2212.10156)
- 240219 - Tsinghua - DriveVLM - The Convergence of Autonomous Driving and Large Vision-Language Models - [arXiv](https://arxiv.org/abs/2402.12289)
---
## Acknowledgement

---
## ‚≠ê

[![Star History Chart](https://api.star-history.com/svg?repos=Jingkou1012/LLM-Study)](https://star-history.com/#Jingkou1012/LLM-Study)
